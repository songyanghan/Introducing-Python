{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Independent citation counter\n",
        "\n",
        "In this notebook, you can calculate the number of independent citations for all of your papers.\n",
        "\n",
        "### What the code will do?\n",
        "For each entry in your Google Scholar profile, the code will output your independent citation count, total citation count and a link to access your independent citation counts.\n",
        "\n",
        "\n",
        "**Sample output:**\n",
        "\n",
        "> The impact of cosmic variance on simulating weak lensing surveys\n",
        ">\n",
        "> Citations: 9/15\n",
        ">\n",
        "> Link:  [http://scholar.google.com/scholar?cites=17631820148925503603&scipsc=1&q=-author:%27A%20Kannawadi%27+-author:%27R%20Mandelbaum%27+-author:%27C%20Lackner%27](http://scholar.google.com/scholar?cites=17631820148925503603&scipsc=1&q=-author:%27A%20Kannawadi%27+-author:%27R%20Mandelbaum%27+-author:%27C%20Lackner%27)\n",
        "\n",
        "The first line is the title of the paper, which has 9 independent citations and 15 total citations.\n",
        "The link takes you to the Google Scholar page with the independent citations.\n",
        "\n",
        "**Note:**\n",
        "Even if the program is unable to fetch independent citation counts, it will still output your total citations and provide a link to access your independent citations.\n",
        "\n",
        "\n",
        "### How to use?\n",
        "In the cell below, replace `qc6CJjYAAAAJ` with your Google Scholar profile ID.\n",
        "You may also want to specify a proxy type (more details below).\n",
        "Then, run all cells.\n",
        "\n",
        "If you are not familiary with running Jupyter notebooks, you can watch an end-to-end video tutorial [here](https://www.youtube.com/watch?v=V13Fx7GesEQ)\n",
        "\n",
        "### Troubleshooting\n",
        "If you see a `MaxTriesExceededException`, it means Google Scholar caught a whiff of your action.\n",
        "Try again later, or use a better proxy.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Enter your Google Scholar profile ID\n",
        "*unless you are Albert Einstein.*\n",
        "\n",
        "For example, if your Google Scholar profile URL is [`https://scholar.google.com/citations?user=qc6CJjYAAAAJ&hl=en`](https://scholar.google.com/citations?user=qc6CJjYAAAAJ&hl=en), then your profile ID is `qc6CJjYAAAAJ`."
      ],
      "metadata": {
        "id": "1bptgjgUD9zX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# The only cell which you are expected to modify.\n",
        "scholar_id = 'StIsMNgAAAAJ'\n",
        "\n",
        "# `proxy_type` must be one of ScraperAPI, Luminati, FreeProxy, SingleProxy or NoProxy.\n",
        "# NoProxy will give only the links to independent, not the counts.\n",
        "proxy_type = 'ScraperAPI'  # Case insensitive"
      ],
      "outputs": [],
      "metadata": {
        "id": "dH6LDl4jD9zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More on `proxy_type`\n",
        "\n",
        "By default, the code provides only the links to page containing independent citations, and does not open the page to count them.\n",
        "Google Scholar actively blocks automated requests to its citation database.\n",
        "Continuous, repeated requests from a single IP address may lead to a ban.\n",
        "However, if you need the counts, you may be able to circumvent this by using a proxy.\n",
        "Below are a few options:\n",
        "\n",
        "- **FreeProxy**: Use continuously changing proxies for free.\n",
        "\n",
        "    This protects your IP address, but is not very effective at circumventing Google Scholar's anti-bot prevention. You might want to use other options if you are unable to reach Google Scholar.\n",
        "\n",
        "\n",
        "- **ScraperAPI** (recommended): [Create a free account](https://www.scraperapi.com/) without providing personal and payment information. Free account supports 5000 requests per month, more that sufficient to run this notebook for most researchers.\n",
        "\n",
        "- **Luminati** (untested): Similar to ScraperAPI, and is known to circumvent Google Scholar's anti-bot prevention better. No free account is available.\n",
        "\n",
        "- **SingleProxy**: Use a single proxy for all requests.\n",
        "\n",
        "- **NoProxy** (default): Using `NoProxy` will not fetch the counts by default. You can still try to fetch the counts (at your own risk) by setting `links_only` below to `False`. Use this sparingly if `FreeProxy` does not work and you don't want to create any accounts. You may also use this safely if you are already connected to a VPN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Read the [official scholarly documentation](https://scholarly.readthedocs.io/en/latest/quickstart.html#using-proxies) for more details."
      ],
      "metadata": {
        "id": "5fquUSCAD9zZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "links_only = (proxy_type.lower() == 'noproxy')"
      ],
      "outputs": [],
      "metadata": {
        "id": "kySHecBJD9zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import the required packages."
      ],
      "metadata": {
        "id": "Z33GhEmaD9zZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "! pip install -q scholarly"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "metadata": {
        "id": "YMQtcAZBD9zZ",
        "outputId": "f9eba08e-f2e1-4704-ed65-7232d5832bb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Suppress the cell's output since it prints a misleading traceback.\n",
        "%%capture\n",
        "try:\n",
        "    from scholarly import scholarly, ProxyGenerator, MaxTriesExceededException\n",
        "except IndexError:\n",
        "    \"\"\" Ignore the harmless IndexError occuring from a dependency\"\"\"\n",
        "    pass\n",
        "import time, random\n",
        "from getpass import getpass\n",
        "try:\n",
        "    from urllib import quote  # type: ignore ; Python 2\n",
        "except ImportError:\n",
        "    from urllib.parse import quote  # type: ignore ; Python 3"
      ],
      "outputs": [],
      "metadata": {
        "id": "yYTtf-tvD9zZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Utility functions\n",
        "def set_proxy(proxy_type='NoProxy'):\n",
        "    \"\"\"Set a proxy for to scrape Google Scholar.\n",
        "\n",
        "    Only `NoProxy`, `FreeProxy` and `ScraperAPI` have been tested.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    proxy_type : str, optional\n",
        "        Type of proxy to use. Case insensitive. Options are:\n",
        "        `ScraperAPI`, `Luminati`, `FreeProxy`, `SingleProxy` and\n",
        "        `NoProxy` (default).\n",
        "    \"\"\"\n",
        "    if proxy_type.lower() == 'noproxy':\n",
        "        print(\"Using no proxies!\")\n",
        "        return\n",
        "\n",
        "    pg = ProxyGenerator()\n",
        "    if proxy_type.lower() == 'scraperapi':\n",
        "        payload = {'api_key': getpass(\"Enter your ScraperAPI key:\"), }\n",
        "        proxy_works = pg.ScraperAPI(payload['api_key'])\n",
        "        if proxy_works is True:\n",
        "            print(\"Using ScraperAPI!\")\n",
        "        elif proxy_works is False:\n",
        "            print(\"ScraperAPI is not working!\")\n",
        "        elif proxy_works is None:\n",
        "            print(\"Changes have not been reflected\")\n",
        "        else:\n",
        "            print(\"God knows what is going on\", proxy_works)\n",
        "    elif proxy_type.lower() == 'luminati':\n",
        "        pg.Luminati(getpass(\"Enter your Luminati username:\"), getpass(\"Enter your Luminati password:\"))\n",
        "        print(\"Using Luminati!\")\n",
        "    elif proxy_type.lower() == 'singleproxy':\n",
        "        proxy_address = getpass(\"Enter your proxy address:\")\n",
        "        pg.SingleProxy(proxy_address, proxy_address)\n",
        "        print(f\"Using SingleProxy: {proxy_address}\")\n",
        "    else:\n",
        "        pg.FreeProxies()\n",
        "        print(\"Using FreeProxy!\")\n",
        "\n",
        "    scholarly.use_proxy(pg)\n",
        "\n",
        "def standardize_names(name):\n",
        "    if not \" \" in name:\n",
        "        return name\n",
        "    try:\n",
        "        parts = name.split(' ')\n",
        "        firstname, lastname = parts[0], parts[-1]\n",
        "        initial = firstname[0]\n",
        "        return quote(f\"'{initial} {lastname}'\")\n",
        "    except:\n",
        "        # This usually happens for collaboration papers\n",
        "        print(f\"Cannot split '{name}' into initial and last names!\")\n",
        "        return quote(f\"{name}\")\n",
        "\n",
        "\n",
        "def fill_independent_citations(publication, links_only=True):\n",
        "    if not publication[\"source\"].name == \"AUTHOR_PUBLICATION_ENTRY\":\n",
        "        raise TypeError(\"Input source must be from a Google Scholar profile page\")\n",
        "\n",
        "    if not publication[\"filled\"]:  # TODO: Don't fill once the patch comes through\n",
        "        scholarly.fill(publication)\n",
        "\n",
        "    citedby_url = publication.get(\"citedby_url\", None)\n",
        "    if citedby_url is None:\n",
        "        # If there are no citations, then there is nothing to do\n",
        "        publication[\"num_independent_citations\"] = 0\n",
        "        return None\n",
        "\n",
        "    author_names = publication[\"bib\"][\"author\"].split(\" and \")\n",
        "    independent_query = \"+\".join([f\"-author:{standardize_names(name)}\" for name in author_names])\n",
        "    independent_url = citedby_url+\"&hl=en&scipsc=1&q=\"+independent_query\n",
        "    publication[\"independent_url\"] = independent_url\n",
        "\n",
        "    if links_only:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        search_results = scholarly.search_pubs_custom_url(independent_url)\n",
        "        num_independent_citations = search_results.total_results if search_results.total_results else 0\n",
        "    except Exception as err:\n",
        "        num_independent_citations = -99\n",
        "\n",
        "    publication[\"num_independent_citations\"] = num_independent_citations"
      ],
      "outputs": [],
      "metadata": {
        "id": "-aMAIVljD9zZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "set_proxy(proxy_type)\n",
        "scholar = scholarly.search_author_id(scholar_id)\n",
        "scholarly.fill(scholar, sections=['basics', 'publications'])\n",
        "scholar_name = scholar[\"name\"]\n",
        "print(f\"Hello {scholar_name} !\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your ScraperAPI key:··········\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-eec52842a1bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscholar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscholarly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_author_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholar_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscholarly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'basics'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'publications'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscholar_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscholar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hello {scholar_name} !\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-62c6ab97c778>\u001b[0m in \u001b[0;36mset_proxy\u001b[0;34m(proxy_type)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproxy_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'scraperapi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'api_key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your ScraperAPI key:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mproxy_works\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScraperAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproxy_works\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using ScraperAPI!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scholarly/_proxy_generator.py\u001b[0m in \u001b[0;36mScraperAPI\u001b[0;34m(self, API_KEY, country_code, premium, render)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mproxy_works\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{prefix}:{API_KEY}@proxy-server.scraperapi.com:8001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mproxy_works\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'http://'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{prefix}:{API_KEY}@proxy-server.scraperapi.com:8001\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scholarly/_proxy_generator.py\u001b[0m in \u001b[0;36m_use_proxy\u001b[0;34m(self, http, https)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxy_works\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxy_works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scholarly/_proxy_generator.py\u001b[0m in \u001b[0;36m_new_session\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;31m# https://www.scraperapi.com/documentation/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"verify\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_webdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
          ]
        }
      ],
      "metadata": {
        "id": "0VOzwGPuD9za",
        "outputId": "90d71dd5-512f-48ba-d573-286cd7533531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Loop over publications\n",
        "if links_only:\n",
        "    print(\"Fetching the independent citation counts has been turned off for your own good\"\n",
        "          \" because you are not using a proxy.\"\n",
        "          \" You can turn it back on at your own risk by explicitly setting `links_only` to `False`.\"\n",
        "          )\n",
        "else:\n",
        "    print(\"You are fetching the counts in addition to the links. The code will run slow intentionally.\")\n",
        "\n",
        "independent_citation_counts = []\n",
        "for paper in scholar[\"publications\"]:\n",
        "    if not links_only:\n",
        "        # Sleep for some random time to mimic human behavior\n",
        "        time.sleep(random.uniform(2, 5))\n",
        "\n",
        "    try:\n",
        "        if paper.get(\"num_independent_citations\", -1) < 0:\n",
        "            fill_independent_citations(paper, links_only=links_only)\n",
        "            independent_citation_counts.append(paper.get(\"num_independent_citations\", 0))\n",
        "    except MaxTriesExceededException as err:\n",
        "        print(\"Google Scholar is aggressively blocking us! Quitting for now.\")\n",
        "        print(err)\n",
        "    finally:\n",
        "        print(\"\\n ------\\n\")\n",
        "        print(paper[\"bib\"][\"title\"])\n",
        "        print(f\"Citations: {paper.get('num_independent_citations', 'NA')}/{paper.get('num_citations')}\")\n",
        "        independent_url = paper.get(\"independent_url\", None)\n",
        "        if independent_url:\n",
        "            print(\"Link: \", \"http://scholar.google.com\"+independent_url)\n",
        "\n",
        "print(\"\\n --- End of list ---\")\n",
        "\n",
        "if not links_only:\n",
        "    print(\"Total number of independent citations = \", sum(independent_citation_counts))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "_3u1DV_hD9za"
      }
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('.venv': pipenv)"
    },
    "interpreter": {
      "hash": "a4e0abcddf1cacd10bcad43358dc25a202947afed0583669c24dee9c22954977"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}