{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hypterparameters\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.01                   # learning rate\n",
    "BUFFER_CAPACITY = 1000000     # capacity of replay buffer, integer!\n",
    "TARGET_REPLACE_ITER = 100   # regulate frequency to update the target network\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, history_len, pred_horizon):\n",
    "        self.learn_step_counter = 0     # counter to update target network\n",
    "\n",
    "        self.rl_net = RLNetwork(history_len, pred_horizon).to(device)\n",
    "        self.target_net = RLNetwork(history_len, pred_horizon).to(device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.rl_net.parameters(), lr=LR)\n",
    "        self.value_loss_func = nn.MSELoss()\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "\n",
    "    def choose_action(self, state, hidden):\n",
    "        \"\"\"\n",
    "        select an action based on the current state and hidden\n",
    "        return action and next_hidden\n",
    "        \"\"\"\n",
    "        state = tuple([torch.FloatTensor(state_element).unsqueeze(\n",
    "            0).to(device) for state_element in state])\n",
    "        hidden = torch.FloatTensor(hidden).unsqueeze(0).to(device)\n",
    "        action, next_hidden, _ = self.rl_net.forward(\n",
    "            state, rl=True, hidden=hidden)\n",
    "        return action.detach().cpu().numpy()[0], next_hidden.detach().cpu().numpy()[0]\n",
    "\n",
    "    def learn(self, gamma=0.99, min_value=-np.inf, max_value=np.inf):\n",
    "        \"\"\"update actor and critic network\"\"\"\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        state, hidden, action, reward, next_state, next_hidden, done = \\\n",
    "            self.replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        hidden = torch.FloatTensor(hidden).to(device)\n",
    "        next_hidden = torch.FloatTensor(next_hidden).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "        done = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "        pred_traj, _, _ = self.rl_net(state, rl=True, hidden=hidden)\n",
    "        _, _, policy_loss = self.rl_net(\n",
    "            state, rl=True, action=pred_traj[:, 0, :])\n",
    "        policy_loss = -policy_loss.mean()\n",
    "\n",
    "        next_pred_traj, _, _ = self.target_net(\n",
    "            next_state, rl=True, hidden=next_hidden)\n",
    "        next_action = next_pred_traj[:, 0, :]\n",
    "        _, _, target_value = self.target_net(\n",
    "            next_state, rl=True, action=next_action.detach())\n",
    "        expected_value = reward + (1.0 - done) * gamma * target_value\n",
    "        expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "        _, _, value = self.rl_net(state, rl=True, action=action[:, 0, :])\n",
    "        value_loss = self.value_loss_func(value, expected_value.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        value_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target net\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.rl_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"save model\"\"\"\n",
    "        torch.save(self.rl_net.state_dict(), filename + \"_net.pt\")\n",
    "        torch.save(self.optimizer.state_dict(), filename + \"_optimizer.pt\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"load model\"\"\"\n",
    "        self.rl_net.load_state_dict(torch.load(filename + \"_net.pt\"))\n",
    "        self.optimizer.load_state_dict(torch.load(filename + \"_optimizer.pt\"))\n",
    "        self.target_net = copy.deepcopy(self.rl_net)\n",
    "\n",
    "\n",
    "class TD3(object):\n",
    "    def __init__(self, history_len, pred_horizon, lr=3e-4):\n",
    "        self.learn_step_counter = 0     # counter to update target network\n",
    "\n",
    "        self.rl_net1 = RLNetwork(history_len, pred_horizon).to(device)\n",
    "        self.rl_net2 = RLNetwork(history_len, pred_horizon).to(device)\n",
    "        self.target_net1 = RLNetwork(history_len, pred_horizon).to(device)\n",
    "        self.target_net2 = RLNetwork(history_len, pred_horizon).to(device)\n",
    "\n",
    "        self.target_net1.load_state_dict(self.rl_net1.state_dict())\n",
    "        self.target_net2.load_state_dict(self.rl_net2.state_dict())\n",
    "\n",
    "        self.optimizer1 = torch.optim.Adam(self.rl_net1.parameters(), lr=lr)\n",
    "        self.optimizer2 = torch.optim.Adam(self.rl_net2.parameters(), lr=lr)\n",
    "        self.value_loss_func = nn.MSELoss()\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "\n",
    "    def choose_action(self, state, hidden):\n",
    "        \"\"\"\n",
    "        select an action based on the current state and hidden\n",
    "        return action and next_hidden\n",
    "        \"\"\"\n",
    "        state = tuple([torch.FloatTensor(state_element).unsqueeze(\n",
    "            0).to(device) for state_element in state])\n",
    "        hidden = torch.FloatTensor(hidden).unsqueeze(0).to(device)\n",
    "        action, next_hidden, _ = self.rl_net1.forward(\n",
    "            state, rl=True, hidden=hidden)\n",
    "        return action.detach().cpu().numpy()[0], next_hidden.detach().cpu().numpy()[0]\n",
    "\n",
    "    def learn(self, gamma=0.99, soft_tau=0.005, noise_std=0.2, noise_clip=0.5, policy_update=2):\n",
    "        \"\"\"update actor and critic network\"\"\"\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        state, hidden, action, reward, next_state, next_hidden, done = \\\n",
    "            self.replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        hidden = torch.FloatTensor(hidden).to(device)\n",
    "        next_hidden = torch.FloatTensor(next_hidden).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "        done = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "        # target policy smoothing\n",
    "        next_pred_traj, _, _ = self.target_net1(\n",
    "            next_state, rl=True, hidden=next_hidden)\n",
    "        next_action = next_pred_traj[:, 0, :]\n",
    "        noise = torch.normal(torch.zeros(\n",
    "            next_action.size()), noise_std).to(device)\n",
    "        noise = torch.clamp(noise, -noise_clip, noise_clip)\n",
    "        next_action += noise\n",
    "\n",
    "        # clipped double-Q learning\n",
    "        _, _, target_value1 = self.target_net1(\n",
    "            next_state, rl=True, action=next_action.detach())\n",
    "        _, _, target_value2 = self.target_net2(\n",
    "            next_state, rl=True, action=next_action.detach())\n",
    "        target_value = torch.min(target_value1, target_value2)\n",
    "        expected_value = reward + (1.0 - done) * gamma * target_value\n",
    "\n",
    "        _, _, value1 = self.rl_net1(state, rl=True, action=action[:, 0, :])\n",
    "        value_loss1 = self.value_loss_func(value1, expected_value.detach())\n",
    "        _, _, value2 = self.rl_net1(state, rl=True, action=action[:, 0, :])\n",
    "        value_loss2 = self.value_loss_func(value2, expected_value.detach())\n",
    "\n",
    "        self.optimizer1.zero_grad()\n",
    "        value_loss1.backward()\n",
    "        self.optimizer1.step()\n",
    "\n",
    "        self.optimizer2.zero_grad()\n",
    "        value_loss2.backward()\n",
    "        self.optimizer2.step()\n",
    "\n",
    "        # delayed update\n",
    "        if self.learn_step_counter % policy_update == 0:\n",
    "            pred_traj, _, _ = self.rl_net1(state, rl=True, hidden=hidden)\n",
    "            _, _, policy_loss = self.rl_net1(\n",
    "                state, rl=True, action=pred_traj[:, 0, :])\n",
    "            policy_loss = -policy_loss.mean()\n",
    "\n",
    "            self.optimizer1.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer1.step()\n",
    "\n",
    "            self.soft_update(self.rl_net1, self.target_net1, soft_tau=soft_tau)\n",
    "            self.soft_update(self.rl_net2, self.target_net2, soft_tau=soft_tau)\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "    def soft_update(self, net, target_net, soft_tau=1e-2):\n",
    "        for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"save model\"\"\"\n",
    "        torch.save(self.rl_net1.state_dict(), filename + \"_net1.pt\")\n",
    "        torch.save(self.optimizer1.state_dict(), filename + \"_optimizer1.pt\")\n",
    "\n",
    "        torch.save(self.rl_net2.state_dict(), filename + \"_net2.pt\")\n",
    "        torch.save(self.optimizer2.state_dict(), filename + \"_optimizer2.pt\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"load model\"\"\"\n",
    "        self.rl_net1.load_state_dict(torch.load(filename + \"_net1.pt\"))\n",
    "        self.optimizer1.load_state_dict(\n",
    "            torch.load(filename + \"_optimizer1.pt\"))\n",
    "        self.target_net1 = copy.deepcopy(self.rl_net1)\n",
    "\n",
    "        self.rl_net2.load_state_dict(torch.load(filename + \"_net2.pt\"))\n",
    "        self.optimizer2.load_state_dict(\n",
    "            torch.load(filename + \"_optimizer2.pt\"))\n",
    "        self.target_net2 = copy.deepcopy(self.rl_net2)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.replay_buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, state, hidden, action, reward, next_state, next_hidden, done):\n",
    "        \"\"\"Saves a transition experience\"\"\"\n",
    "        if len(self.replay_buffer) < self.capacity:\n",
    "            self.replay_buffer.append(None)\n",
    "        self.replay_buffer[self.position] = (state, hidden, action, reward, next_state,\n",
    "                                             next_hidden, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.replay_buffer, batch_size)\n",
    "        state, hidden, action, reward, next_state, next_hidden, done = \\\n",
    "            map(np.stack, zip(*batch))\n",
    "        return state, hidden, action, reward, next_state, next_hidden, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.replay_buffer)\n",
    "\n",
    "\n",
    "class RLNetwork(nn.Module):\n",
    "    \"\"\"modified from TrajectoryImitationCNNFCLSTM\"\"\"\n",
    "\n",
    "    def __init__(self, history_len, pred_horizon, embed_size=64,\n",
    "                 hidden_size=128, cnn_net=models.mobilenet_v2,\n",
    "                 pretrained=True, num_actions=4):\n",
    "        super(RLNetwork, self).__init__()\n",
    "        self.compression_cnn_layer = nn.Conv2d(12, 3, 3, padding=1)\n",
    "        self.cnn = cnn_net(pretrained=pretrained)\n",
    "        self.cnn_out_size = 1000\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.history_len = history_len\n",
    "        self.pred_horizon = pred_horizon\n",
    "\n",
    "        self.embedding_fc_layer = torch.nn.Sequential(\n",
    "            nn.Linear(4, embed_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.h0, self.c0, self.lstm = self.generate_lstm(embed_size, hidden_size)\n",
    "\n",
    "        self.output_fc_layer = torch.nn.Sequential(\n",
    "            nn.Linear(hidden_size + self.cnn_out_size, 4),\n",
    "        )\n",
    "\n",
    "        # the following layers belong to the value network branch\n",
    "        self.valuenet_fc1_layer = nn.Linear(\n",
    "            self.cnn_out_size + num_actions, hidden_size)\n",
    "        self.valuenet_fc2_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.valuenet_fc3_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, X, rl=False, hidden=0, action=[[0, 0, 0, 0]]):\n",
    "        img_feature, hist_points, hist_points_step = X\n",
    "        batch_size = img_feature.size(0)\n",
    "        if rl:\n",
    "            h0, c0 = hidden[0], hidden[1]\n",
    "        else:\n",
    "            # manually add the unsqueeze before repeat to avoid onnx to tensorRT parsing error\n",
    "            h0 = self.h0.unsqueeze(0)   # size: 1, hidden_size\n",
    "            c0 = self.c0.unsqueeze(0)\n",
    "        ht, ct = h0.repeat(1, batch_size, 1),\\\n",
    "            c0.repeat(1, batch_size, 1)\n",
    "\n",
    "        img_embedding = self.cnn(\n",
    "            self.compression_cnn_layer(img_feature)).view(batch_size, -1)\n",
    "        pred_traj = torch.zeros(\n",
    "            (batch_size, 1, 4), device=img_feature.device)\n",
    "\n",
    "        for t in range(1, self.history_len + self.pred_horizon):\n",
    "            if t < self.history_len:\n",
    "                cur_pose_step = hist_points_step[:, t, :].float()\n",
    "                cur_pose = hist_points[:, t, :].float()\n",
    "            else:\n",
    "                pred_input = torch.cat(\n",
    "                    (ht.view(batch_size, -1), img_embedding), 1)\n",
    "                cur_pose_step = self.output_fc_layer(\n",
    "                    pred_input).float().clone()\n",
    "                cur_pose = cur_pose + cur_pose_step\n",
    "                # dim of the pred_traj: batch, time horizon, states dimension\n",
    "                # state: (dx, dy, dheading, speed)\n",
    "                pred_traj = torch.cat(\n",
    "                    (pred_traj, cur_pose.clone().unsqueeze(1)), dim=1)\n",
    "\n",
    "            disp_embedding = self.embedding_fc_layer(\n",
    "                cur_pose_step.clone()).view(batch_size, 1, -1)\n",
    "\n",
    "            _, (ht, ct) = self.lstm(disp_embedding, (ht, ct))\n",
    "\n",
    "        # the following calculates the output for value network branch\n",
    "        # here the action only includes the first point of pred_traj\n",
    "        x = torch.cat([img_embedding, action], 1)\n",
    "        x = F.relu(self.valuenet_fc1_layer(x))\n",
    "        x = F.relu(self.valuenet_fc2_layer(x))\n",
    "        x = self.valuenet_fc3_layer(x)\n",
    "\n",
    "        return pred_traj[:, 1:, :], (ht, ct), x\n",
    "    \n",
    "    def generate_lstm(self, input_size, hidden_size, bilateral=False):\n",
    "        h0 = torch.zeros(1, hidden_size)\n",
    "        c0 = torch.zeros(1, hidden_size)\n",
    "        nn.init.xavier_normal_(h0, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_normal_(c0, gain=nn.init.calculate_gain('relu'))\n",
    "        h0 = nn.Parameter(h0, requires_grad=True)\n",
    "        c0 = nn.Parameter(c0, requires_grad=True)\n",
    "        lstm = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        return h0, c0, lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the RLnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_len = 10\n",
    "pred_horizon = 10\n",
    "hidden_size = 128\n",
    "\n",
    "# initiate the RL framework\n",
    "net = RLNetwork(history_len, pred_horizon, hidden_size=hidden_size)\n",
    "\n",
    "img_feature, hist_points, hist_points_step = torch.rand(1, 12, 200, 200), torch.rand(1, 10, 4), torch.rand(1, 10, 4)\n",
    "X = img_feature, hist_points, hist_points_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-78969eb9b5ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ccb99eaf416c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, rl, hidden, action)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;31m# the following calculates the output for value network branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# here the action only includes the first point of pred_traj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaluenet_fc1_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaluenet_fc2_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-15d2adb54720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'img_embedding' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
